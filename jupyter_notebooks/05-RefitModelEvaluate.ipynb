{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Refit Pipeline, Model and Evaluate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Here, we will refit the pipeline with the most important features\n",
        "* Fit and evaluate the ML pipeline to predict attrition\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Raw dataset in outputs/datasets/collection/employee-attrition.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* TrainSet and TestSet\n",
        "* Data cleaning and feature engineering pipeline\n",
        "* Modeling pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = (pd.read_csv(f\"outputs/datasets/collection/employee-attrition.csv\")\n",
        "        .drop(labels=['DailyRate','EmployeeCount', 'EmployeeNumber', 'HourlyRate',\n",
        "                      'MonthlyRate', 'StandardHours', 'Over18'], axis=1))\n",
        "\n",
        "df['Attrition'] = df['Attrition'].replace({\"Yes\":1, \"No\":0})\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the last notebook, these are the most important features to predict attrition. We will filter the dataset such that it contains only those features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_features_and_target = [\n",
        "                            'OverTime', 'YearsAtCompany', \n",
        "                            'Age', 'JobLevel', 'EnvironmentSatisfaction',\n",
        "                            'Department', 'JobSatisfaction', 'StockOptionLevel',\n",
        "                            'JobInvolvement', 'Attrition'\n",
        "                            ]\n",
        "\n",
        "df = df.filter(best_features_and_target)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will create:\n",
        "* Split the dataset\n",
        "* Data cleaning and feature engineering pipeline\n",
        "* Handle imbalance\n",
        "* Modeling pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Attrition'], axis=1),\n",
        "                                                    df['Attrition'],\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=0,\n",
        "                                                   )\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Pipeline 1: Data cleaning and feature engineering pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the datacleaning and feature engineering pipeline based on the conclusions from the last notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Feature Engineering\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine import transformation as vt\n",
        "\n",
        "\n",
        "def PipelineDataCleaningAndFeatureEngineering():\n",
        "    pipeline_base = Pipeline([\n",
        "        ('yj', vt.YeoJohnsonTransformer(variables=['YearsAtCompany']) ),\n",
        "        ('OrdinalCategoricalEncoder', OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                     variables=['Department', 'OverTime'])),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base\n",
        "\n",
        "\n",
        "PipelineDataCleaningAndFeatureEngineering()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_data_cleaning_feat_eng = PipelineDataCleaningAndFeatureEngineering()\n",
        "X_train = pipeline_data_cleaning_feat_eng.fit_transform(X_train)\n",
        "X_test = pipeline_data_cleaning_feat_eng.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Handle imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recap on the target imbalance mentioned in notebook 02. There is more 'No attrition' or 0's than 'Yes attrition' or 1's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attrition_array = y_train.to_numpy()\n",
        "\n",
        "def plot_target_count(target):\n",
        "    fig, axes = plt.subplots(figsize=(5,5))\n",
        "    sns.countplot(x = target)\n",
        "    plt.show()\n",
        "\n",
        "plot_target_count(attrition_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use Synthetic Minority Over-sampling Technique (SMOTE) from the `imblearn` library. The sampling strategy will be `minority` so it resamples all classes except the majority."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote_over = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = smote_over.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot once again to check after SMOTE operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attrition_array_balanced = y_train.to_numpy()\n",
        "plot_target_count(attrition_array_balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we notice that the traget is now balanced. One has to mind that we changed the dataset for the benefit of fitting a better model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Pipeline2: Modelling and Hyperparameter Optimisation pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, the pipeline consists of:\n",
        "* **StandardScaler**: to rescale the features to have standard normal distribution with zero mean and standard deviation of 1. It is performed on all variables. The variable distribution might be slightly different.\n",
        "* **SelectFromModel**: to select the relevant features for fitting. We use the embedded method to perform feature selection during training. The model will be the algorithm of our choice.\n",
        "* **model**: the ML algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "# ML algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "def PipelineClf(model):\n",
        "    pipeline_base = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "\n",
        "    return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grid search Cross Validation\n",
        "\n",
        "We use the custom python class for hyperparameter optimization provided by CI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "    \"\"\"\n",
        "    Custom class provided by CI\n",
        "    \"\"\"\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model = PipelineClf(self.models[key])\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the best classifier model (GradientBoostingClassifier) with the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = {\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=0),\n",
        "}\n",
        "\n",
        "best_params = {\n",
        "    \"GradientBoostingClassifier\": {\n",
        "        'model__max_depth': [3],\n",
        "        'model__min_samples_split': [2],\n",
        "        'model__min_weight_fraction_leaf': [0.0],\n",
        "        'model__n_estimators': [1000],\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the pipeline to the TrainSet data. We set cross validation `cv` to 5 and we use all processors or threads by setting `n_jobs` to -1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, recall_score\n",
        "search = HyperparameterOptimizationSearch(models=best_model, params=best_params)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the performance summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search_summary, grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "grid_search_summary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grab the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = grid_search_summary.iloc[0,0]\n",
        "pipeline_clf = grid_search_pipelines[best_model].best_estimator_\n",
        "pipeline_clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assess feature importnace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': pipeline_clf['model'].feature_importances_})\n",
        "    .sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "# re-assign best_features order\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features}\")\n",
        "\n",
        "sns.barplot(data=df_feature_importance, x=\"Feature\", y=\"Importance\")\n",
        "plt.xticks(rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the 27 features, we end up with 9 important features with `OverTime` being the variable that strongly affects the target. We will use all 9 features for refitting the model. But first we evaluate the current pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
        "\n",
        "    prediction = pipeline.predict(X)\n",
        "\n",
        "    print('---  Confusion Matrix  ---')\n",
        "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
        "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
        "          ))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print('---  Classification Report  ---')\n",
        "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
        "    print(\"#### Train Set #### \\n\")\n",
        "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
        "\n",
        "    print(\"#### Test Set ####\\n\")\n",
        "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=pipeline_clf,\n",
        "                label_map= ['No Attrition', 'Attrition'] \n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that most of the model success metrics are met. These were:\n",
        "* A minimum of 80% no-attrition on train and test datasets, here we get 99% and 88%, respectively\n",
        "* A minimum of 60% attrition on train and test datasets, here we get 99% and 40%, respectively\n",
        "* Note that the attrition prediction still does not meet the requirements (20% below). Ideally, further iterations with other models and hyperparameters can be tested. However, due to time limitation of the project, we will not proceed with further iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dump the model pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "version = 'v3'\n",
        "file_path = f'outputs/ml_pipelines/{version}'\n",
        "\n",
        "try:\n",
        "    os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.to_csv(f\"{file_path}/X_train.csv\", index=False)\n",
        "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)\n",
        "X_test.to_csv(f\"{file_path}/X_test.csv\", index=False)\n",
        "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dump the pipelines to pickle files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the data cleaning and feature engineering pipeline\n",
        "joblib.dump(value=pipeline_data_cleaning_feat_eng ,\n",
        "            filename=f\"{file_path}/clf_pipeline_data_cleaning_feat_eng.pkl\")\n",
        "# Save the modeling pipeline\n",
        "joblib.dump(value=pipeline_clf ,\n",
        "            filename=f\"{file_path}/clf_pipeline_model.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature importance was one of the business requirement, therefore, we plot it and save the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.barplot(data=df_feature_importance, x=\"Feature\", y=\"Importance\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.savefig(f'{file_path}/features_importance.png', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice that 5 of the 9 variables remained in the same ranking from the last iteration (previous notebook). Namely, the first 4 variables and `Department`. The other 4 variables have shuffled in between each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The final data cleaning and feature engineering as well as modeling pipelines were generated\n",
        "* The model can predict no-attrition reliably but other iteratons or more data in the dataset is needed to predict attrition better"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
