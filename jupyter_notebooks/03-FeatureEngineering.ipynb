{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **FeatureEngineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Further data preparation for the ML algorithm by performing several steps of feature engineering\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Dataset in outputs/datasets/cleaned/TrainSetCleaned.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* No file output but we get a decision on the ML pipeline transformations/encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data does not have missing values, therefore we will not need any imputer. In the last notebook, we already dropped 7 columns from the dataset of 35 columns, leaving 28 columns including the target. The target `Attrition` was transformed from a categorical variable (Yes, No) to a numerical one (1, 0). The operation was done manually and mimics an ordinal encoder.\n",
        "\n",
        "As mentioned before, there are 8 categorical variables (after dropping `Over18`) including `Attrition`. These will be subjected to OrdinalCategoricalEncoder.\n",
        "\n",
        "As for the numerical data, we will have to look on their distribution (QQ plots and histogram)..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet = pd.read_csv(f\"outputs/datasets/cleaned/TrainSetCleaned.csv\")\n",
        "TrainSet.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will perform:\n",
        "* Visualize data distribution to decide which numerical data to apply transformation to\n",
        "* Numerical data transformation\n",
        "* Categorical encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Numerical data distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot all numerical columns and calculate their skewness and kurtosis to see how far they are from normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pingouin as pg\n",
        "\n",
        "def plot_distribution(dataframe):\n",
        "\n",
        "    for col in dataframe.columns:\n",
        "        if dataframe[col].dtype=='int64':\n",
        "            print(f\"*** {col} ***\")\n",
        "            fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4), width_ratios = [1, 1])\n",
        "            sns.histplot(data=dataframe, x=col, ax=axes[0], kde=True, multiple=\"stack\")\n",
        "            qqplot = pg.qqplot(dataframe[col], dist='norm', ax=axes[1])\n",
        "            qqplot.set(xlabel=None)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            print(f\"skewness: {dataframe[col].skew().round(2)} | kurtosis: {dataframe[col].kurtosis().round(2)}\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "plot_distribution(TrainSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We notice there are 11 numerical features which are discrete (no. of unique values <= 7), we will drop these from the analysis to focus on the continuous numerical variables. We will also drop `Age` from the analysis because it was apparent from last notebook, that age distribution was very close to normal distribution.\n",
        "\n",
        "Note: a value of 7 to distinguish discrete from continuous was chosen arbitrarly from looking at figures output from the above cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "apply_transform_to = []\n",
        "for col in TrainSet.drop(labels='Age', axis=1).columns:\n",
        "    if len(TrainSet[col].unique()) > 7 and TrainSet[col].dtype=='int64':\n",
        "        apply_transform_to.append(col)\n",
        "\n",
        "df_continuous = TrainSet.filter(apply_transform_to)\n",
        "plot_distribution(df_continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also check for outliers. We observe a large number of outliers in `MonthlyIncome`, `TotalWorkingYears`, `YearsAtCompany` and `YearsSinceLastPromotion`. Ideally, we can apply a transformer like Winsorizer but for now, we will leave it as it is (as will be shown later that other 'numerical' transformers will take care of that)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histogram_and_boxplot(df):\n",
        "  for col in df.columns:\n",
        "    fig, axes = plt.subplots(nrows=2 ,ncols=1 ,figsize=(6,6), gridspec_kw={\"height_ratios\": (.15, .85)})\n",
        "    sns.boxplot(data=df, x=col, ax=axes[0])\n",
        "    sns.histplot(data=df, x=col, kde=True, ax=axes[1])\n",
        "    fig.suptitle(f\"{col} Distribution - Boxplot and Histogram\")\n",
        "    plt.show()\n",
        "\n",
        "    IQR = df[col].quantile(q=0.75) - df[col].quantile(q=0.25)\n",
        "    print(\n",
        "        f\"This is the range where a datapoint is not an outlier: from \"\n",
        "        f\"{(df[col].quantile(q=0.25) - 1.5*IQR).round(2)} to \"\n",
        "        f\"{(df[col].quantile(q=0.75) + 1.5*IQR).round(2)}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "plot_histogram_and_boxplot(df_continuous)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Numerical data transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the following function to compare the data distribution before and after the transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_skew_kurtosis(df,col, moment):\n",
        "    \"\"\"\n",
        "    This function is adapted from the feature engineering lesson\n",
        "    \"\"\"\n",
        "    print(f\"{moment}  | skewness: {df[col].skew().round(2)} | kurtosis: {df[col].kurtosis().round(2)}\")\n",
        "\n",
        "def compare_distributions_before_and_after_applying_transformer(df, df_transformed1, df_transformed2, method1, method2, items=None):\n",
        "    if items != None:\n",
        "        df = df.filter(items)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "    for col in df.columns:\n",
        "        print(f\"*** {col} ***\")\n",
        "        fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(8,8))\n",
        "\n",
        "        plot1 = sns.histplot(data=df, x=col, kde=True, ax=axes[0,0])\n",
        "        plot2 = sns.histplot(data=df_transformed1, x=col, kde=True, ax=axes[1,0], color=['green'])\n",
        "        plot3 = sns.histplot(data=df_transformed2, x=col, kde=True, ax=axes[2,0], color=['red'])\n",
        "        plot1.set(xlabel=None)\n",
        "        plot2.set(xlabel=None)\n",
        "        plot1.set_title('Before Transform')\n",
        "        plot2.set_title(method1)\n",
        "        plot3.set_title(method2)\n",
        "        \n",
        "        qqplot1 = pg.qqplot(df[col], dist='norm',ax=axes[0,1])\n",
        "        qqplot2 = pg.qqplot(df_transformed1[col], dist='norm',ax=axes[1,1])\n",
        "        qqplot1.set(xlabel=None)\n",
        "        qqplot2.set(xlabel=None)        \n",
        "        pg.qqplot(df_transformed2[col], dist='norm',ax=axes[2,1])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        calculate_skew_kurtosis(df, col, moment='before transformation')\n",
        "        calculate_skew_kurtosis(df_transformed1, col, moment=f'after {method1}')\n",
        "        calculate_skew_kurtosis(df_transformed2, col, moment=f'after {method2}')\n",
        "        print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can not apply logarithmic, reciprocal and BoxCox transformers since we have zeros in our data. Two exceptions from this are the columns `PercentSalaryHike` and `DistanceFromHome`, which will be treated separately after the following analysis, if it was not successful in getting a normal distribution. Therefore, we proceed with the power transformer and the Yeo Johnson transformer to all numerical variables. \n",
        "\n",
        "We set the pipeline with the transformer and then fit and transform the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine import transformation as vt\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline_power = Pipeline([\n",
        "      ( 'pt', vt.PowerTransformer() )\n",
        "  ])\n",
        "pipeline_YeoJohnson = Pipeline([\n",
        "      ('yj', vt.YeoJohnsonTransformer() )\n",
        "  ])\n",
        "\n",
        "df_transformed_power = pipeline_power.fit_transform(df_continuous)\n",
        "df_transformed_YeoJohnson = pipeline_YeoJohnson.fit_transform(df_continuous)\n",
        "\n",
        "compare_distributions_before_and_after_applying_transformer(df_continuous, df_transformed_power, df_transformed_YeoJohnson, 'Power Transform', 'YeoJohnson Transform')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comments on the numerical transformation:\n",
        "* In all the plots, YeoJohnson transformation performance with superior to the Power transformation when it comes to skewness. That means the data becomes more symmetric at the center point.\n",
        "* As for kurtosis, except for `YearsAtCompany`, YeoJohnson transformation generated data with higher negative kurtosis. Meaning that the distribution has thinner tails than the original data as well as the Power transformed data\n",
        "\n",
        "Generally, the transformed data distribution resembles a normal distribution more than the original data **only for 3 variables**. These are `MonthlyIncome`, `TotalWorkingYears`, `YearsAtCompany`.\n",
        "\n",
        "Therefore, we proceed with YeoJohnson transformation only on these variables in the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we conclude, we run a numerical transformation on `PercentSalaryHike` and `DistanceFromHome` with other transformers. As they have non-zero values, we can perform reciprocal and logarithmic transformations to see if the distribution will get close to a normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_log = Pipeline([\n",
        "      ( 'log', vt.LogTransformer(variables=['PercentSalaryHike','DistanceFromHome']) )\n",
        "  ])\n",
        "pipeline_reciprocal = Pipeline([\n",
        "      ( 'reciprocal', vt.ReciprocalTransformer(variables=['PercentSalaryHike','DistanceFromHome']) )\n",
        "  ])\n",
        "\n",
        "df_transformed_log = pipeline_log.fit_transform(df_continuous)\n",
        "df_transformed_reciprocal = pipeline_reciprocal.fit_transform(df_continuous)\n",
        "\n",
        "compare_distributions_before_and_after_applying_transformer(df_continuous,\n",
        "                                                            df_transformed_log,\n",
        "                                                            df_transformed_reciprocal,\n",
        "                                                            'Log Transform', 'Reciprocal Transform',\n",
        "                                                            items=['PercentSalaryHike','DistanceFromHome'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the distribution does not improve. Finally, we perform the chosen numerical transformation on the chosen variables in the TrainSet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_YeoJohnson = Pipeline([\n",
        "      ('yj', vt.YeoJohnsonTransformer(variables=['MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany']) )\n",
        "  ])\n",
        "\n",
        "TrainSet_transformed_YeoJohnson = pipeline_YeoJohnson.fit_transform(TrainSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we conclude this section, let's look again at the outliers. We see that there has been an improvement after normalizing the distribution with Yeo Johnson transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_histogram_and_boxplot(TrainSet_transformed_YeoJohnson.filter(['MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Ordinal Categorical Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We perform categorical encoding to transform categorical variables into integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "apply_transform_to = []\n",
        "\n",
        "for col in TrainSet.columns:\n",
        "      if TrainSet[col].dtype=='object':\n",
        "            apply_transform_to.append(col)\n",
        "\n",
        "print(f'Variables to apply the transformation to are: {apply_transform_to}')\n",
        "print('\\n')\n",
        "print(f'-------------------Before Transformation--------------------------')\n",
        "print('\\n')\n",
        "# Before transformation\n",
        "for col in TrainSet.filter(apply_transform_to).columns.to_list():\n",
        "      print(f\"{col} \\n{TrainSet[col].unique()} \\n\\n\")\n",
        "\n",
        "# Transforma and fit\n",
        "pipeline = Pipeline([\n",
        "      ('ordinal_encoder', OrdinalEncoder(encoding_method='arbitrary') )\n",
        "])\n",
        "\n",
        "TrainSet_transformed_ordinal = pipeline.fit_transform(TrainSet.filter(apply_transform_to))\n",
        "\n",
        "print(f'-------------------After Transformation--------------------------')\n",
        "print('\\n')\n",
        "# Check that transformation happened\n",
        "for col in TrainSet_transformed_ordinal.filter(apply_transform_to).columns.to_list():\n",
        "      print(f\"{col} \\n{TrainSet_transformed_ordinal[col].unique()} \\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Smart correlated selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We look for groups of features that correlate strongly amongst themselves. A threshold of 0.6 means that any variable correlations that are at least moderate will be considered and subject to removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "pipeline = Pipeline([\n",
        "      ( 'SmartCorrelatedSelection', SmartCorrelatedSelection(method=\"spearman\",\n",
        "                                                             threshold=0.6,\n",
        "                                                             selection_method=\"variance\",))\n",
        "])\n",
        "\n",
        "TrainSet_transformed_smart_correlation = pipeline.fit_transform(TrainSet)\n",
        "\n",
        "# sets of features were marked as correlated\n",
        "pipeline['SmartCorrelatedSelection'].correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline['SmartCorrelatedSelection'].features_to_drop_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check that the transformation occured\n",
        "TrainSet_transformed_smart_correlation.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transformations that will be added to the ML pipeline are:\n",
        "* Ordinal categorical encoding on ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n",
        "* Yeo Johnson numerical transformation on ['MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany']\n",
        "* Smart correlation selection: ['JobLevel', 'PerformanceRating', 'TotalWorkingYears', 'YearsInCurrentRole', 'YearsWithCurrManager']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
